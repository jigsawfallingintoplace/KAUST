{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "English to Spanish",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOybP5kqihoQo16RrlCfW5y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigsawfallingintoplace/KAUST/blob/master/English_to_Spanish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RDWh1g__qFy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "a6f0f051-bc62-46b1-f58b-ff4a6e076a02"
      },
      "source": [
        "!pip install laserembeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting laserembeddings\n",
            "  Downloading https://files.pythonhosted.org/packages/c5/6b/93843d90080666571a79f8eb195fa58aa5e45cf24d36158b9c01dba306e2/laserembeddings-1.0.1-py3-none-any.whl\n",
            "Collecting subword-nmt<0.4.0,>=0.3.6\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: torch<2.0.0,>=1.0.1.post2 in /usr/local/lib/python3.6/dist-packages (from laserembeddings) (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.15.4 in /usr/local/lib/python3.6/dist-packages (from laserembeddings) (1.18.5)\n",
            "Collecting transliterate==1.10.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/6e/9a9d597dbdd6d0172427c8cc07c35736471e631060df9e59eeb87687f817/transliterate-1.10.2-py2.py3-none-any.whl (45kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.3MB/s \n",
            "\u001b[?25hCollecting sacremoses==0.0.35\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n",
            "\u001b[K     |████████████████████████████████| 860kB 6.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2.0.0,>=1.0.1.post2->laserembeddings) (0.16.0)\n",
            "Requirement already satisfied: six>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from transliterate==1.10.2->laserembeddings) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (0.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sacremoses==0.0.35->laserembeddings) (4.41.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=4096e919baa03bdeb24497337bc6679e5b1dd1bc9e9c56e31a87ed36539c41b1\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: subword-nmt, transliterate, sacremoses, laserembeddings\n",
            "Successfully installed laserembeddings-1.0.1 sacremoses-0.0.35 subword-nmt-0.3.7 transliterate-1.10.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AzmXOe5_66p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "ffb0e91d-43a9-46d0-a09a-f0884e5bb99b"
      },
      "source": [
        "!python -m laserembeddings download-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading models into /usr/local/lib/python3.6/dist-packages/laserembeddings/data\n",
            "\n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
            "\n",
            "✨ You're all set!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkR3fa21_-12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "from laserembeddings import Laser\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "\n",
        "laser = Laser()\n",
        "\n",
        "trainfile = \"/content/2018-E-c-En-train.txt\"\n",
        "testfile = \"/content/2018-E-c-Es-test-gold.txt\"\n",
        "devfile = \"/content/2018-E-c-Es-dev.txt\"\n",
        "savepath = \"/content/LASERSentiment.model\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def file_to_data(file):\n",
        "    with open(file) as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        data = list(reader)\n",
        "    return data\n",
        "\n",
        "traindata = file_to_data(trainfile)\n",
        "testdata = file_to_data(testfile)\n",
        "devdata = file_to_data(testfile)\n",
        "\n",
        "\n",
        "def get_all_tweets(data): \n",
        "    return [d[1] for d in data[1:]]\n",
        "\n",
        "def get_label_lists(data):\n",
        "    return [[int(x) for x in d[2:]] for d in data[1:]]\n",
        "\n",
        "def get_label_tensors(data):\n",
        "    label_tensors = []\n",
        "    for d in data[1:]:\n",
        "        tmp = torch.zeros(11)\n",
        "        for i in range(11):\n",
        "            if d[2 + i] == '1':\n",
        "                tmp[i] = 1\n",
        "        label_tensors.append(tmp)\n",
        "    return label_tensors\n",
        "\n",
        "train_tweets = get_all_tweets(traindata)\n",
        "train_embeddings = laser.embed_sentences(train_tweets, lang='en')\n",
        "test_tweets = get_all_tweets(testdata)\n",
        "test_embeddings = laser.embed_sentences(test_tweets, lang='es')\n",
        "dev_tweets = get_all_tweets(devdata)\n",
        "dev_embeddings = laser.embed_sentences(dev_tweets, lang='es')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCGmzqRQiDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.layer_1 = nn.Linear(1024, 512)\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.layer_3 = nn.Linear(512, 512)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(512)\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3zWGvug_YWlg",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.layer_4 = nn.Linear(512, 512)\n",
        "        self.layer_out = nn.Linear(512, 11) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(512)\n",
        "    def forward(self, x):\n",
        "        x = self.layer_4(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_out(x)\n",
        "        # x = nn.Softmax(dim=1)(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7H91PO8YX7O",
        "colab": {}
      },
      "source": [
        "class LanguageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageClassifier, self).__init__()\n",
        "        self.layer_4 = nn.Linear(512, 512)\n",
        "        self.layer_out = nn.Linear(512, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(512)\n",
        "    def forward(self, x):\n",
        "        x = self.layer_4(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_out(x)\n",
        "        # x = nn.Softmax(dim=1)(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xmRErUuUNs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_embeddings_tensors = torch.from_numpy(train_embeddings)\n",
        "label_tensors = get_label_tensors(traindata)\n",
        "assert len(train_embeddings_tensors) == len(label_tensors)\n",
        "\n",
        "train_dataset = TensorDataset(train_embeddings_tensors, torch.stack(label_tensors))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "train_iter_source = iter(train_dataloader)\n",
        "\n",
        "\n",
        "test_label_tensors = get_label_tensors(testdata)\n",
        "y_true = torch.stack(test_label_tensors)\n",
        "\n",
        "test_dataset = TensorDataset(torch.from_numpy(test_embeddings), y_true)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "train_iter_target = iter(test_dataloader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCPR6A1aZWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "lambd = 0.01\n",
        "learning_rate = 0.005\n",
        "Q_learning_rate = 0.005\n",
        "F = FeatureExtractor()\n",
        "P = SentimentClassifier()\n",
        "Q = LanguageClassifier()\n",
        "F, P, Q = F.to(device), P.to(device), Q.to(device)\n",
        "optimizer = optim.Adam(list(F.parameters()) + list(P.parameters()), lr=learning_rate)\n",
        "q_optimizer = optim.Adam(Q.parameters(), lr=Q_learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e1ce3Wso016",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# net.load_state_dict(torch.load(savepath))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYz-KclCWufJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freeze(net):\n",
        "    for p in net.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def unfreeze(net):\n",
        "    for p in net.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def get_batch_source():\n",
        "  global train_iter_source\n",
        "  try:\n",
        "    return next(train_iter_source)\n",
        "  except:\n",
        "    train_iter_source = iter(train_dataloader)\n",
        "    return next(train_iter_source)\n",
        "\n",
        "def get_batch_target():\n",
        "  global train_iter_target\n",
        "  try:\n",
        "    return next(train_iter_target)\n",
        "  except:\n",
        "    train_iter_target = iter(test_dataloader)\n",
        "    return next(train_iter_target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5XrSnHjWIIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "770d358f-6069-467c-9f76-6b56b6d5c546"
      },
      "source": [
        "from sklearn.metrics import label_ranking_average_precision_score, f1_score, jaccard_score\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=60, verbose=True)\n",
        "\n",
        "\n",
        "dev_label_tensors = get_label_tensors(devdata)\n",
        "dev_y_true = torch.stack(dev_label_tensors)\n",
        "up = 0\n",
        "\n",
        "dev_dataset = TensorDataset(torch.from_numpy(dev_embeddings), dev_y_true)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "loss_diagram = []\n",
        "num_epochs = 100\n",
        "q_iter = 5\n",
        "cnt = 0\n",
        "clip_lower = -0.01\n",
        "clip_upper = 0.01\n",
        "running_loss = 0\n",
        "jac = 0\n",
        "while (jac < 0.45):\n",
        "    freeze(P)\n",
        "    freeze(F)\n",
        "    unfreeze(Q)\n",
        "    for q in range(q_iter):\n",
        "      for p in Q.parameters():\n",
        "        p.data.clamp_(clip_lower, clip_upper)\n",
        "      \n",
        "      Q.zero_grad()\n",
        "\n",
        "      X_source, _ = get_batch_source()\n",
        "      X_target, _ = get_batch_target()\n",
        "      feature_source = F(X_source)\n",
        "      feature_target = F(X_target)\n",
        "      loss_q = torch.mean(-Q(feature_source)) + torch.mean(Q(feature_target))\n",
        "      q_optimizer.zero_grad()\n",
        "      loss_q.backward()\n",
        "      q_optimizer.step()\n",
        "\n",
        "    unfreeze(F)\n",
        "    unfreeze(P)\n",
        "    freeze(Q)\n",
        "\n",
        "    for p in Q.parameters():\n",
        "        p.data.clamp_(clip_lower, clip_upper) \n",
        "\n",
        "    F.zero_grad()\n",
        "    P.zero_grad()\n",
        "\n",
        "    X_source, Y_source = get_batch_source()\n",
        "    X_target, Y_target = get_batch_target()\n",
        "    feature_source = F(X_source)\n",
        "    sentiment_source = P(feature_source)\n",
        "    language_source = Q(feature_source)\n",
        "\n",
        "    feature_target = F(X_target)\n",
        "    language_target = Q(feature_target)\n",
        "\n",
        "    loss_sentiment = loss_function(sentiment_source, Y_source)\n",
        "    loss_sentiment.backward(retain_graph=True)\n",
        "    loss_language = lambd*(torch.mean(language_source) - torch.mean(language_target))\n",
        "    loss_language.backward(retain_graph=True)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "    all_list = []\n",
        "    for x, y in dev_dataloader:\n",
        "      guess = P(F(x))\n",
        "      all_list.append(guess)\n",
        "    concat_tensor = torch.cat(all_list)\n",
        "    dev_y_score = concat_tensor.detach().numpy()\n",
        "    dev_y_pred = dev_y_score.copy()\n",
        "    for i in range(dev_y_pred.shape[0]):\n",
        "      for j in range(dev_y_pred.shape[1]):\n",
        "        if (dev_y_pred[i][j] >= 0.5):\n",
        "          dev_y_pred[i][j] = 1\n",
        "        else:\n",
        "          dev_y_pred[i][j] = 0\n",
        "    jac = jaccard_score(dev_y_true, dev_y_pred, average='samples')\n",
        "    print(\"Jaccard Score:\", jac)\n",
        "    scheduler.step(jac)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard Score: 0.026512497080121464\n",
            "Jaccard Score: 0.03907965428638169\n",
            "Jaccard Score: 0.07431090866619948\n",
            "Jaccard Score: 0.07753445456669002\n",
            "Jaccard Score: 0.07849801448259752\n",
            "Jaccard Score: 0.12477224947442188\n",
            "Jaccard Score: 0.154350619014249\n",
            "Jaccard Score: 0.17526278906797474\n",
            "Jaccard Score: 0.1680506890913338\n",
            "Jaccard Score: 0.15074164914739546\n",
            "Jaccard Score: 0.13441952814762903\n",
            "Jaccard Score: 0.13859495444989486\n",
            "Jaccard Score: 0.13517285680915672\n",
            "Jaccard Score: 0.1383204858677879\n",
            "Jaccard Score: 0.14006073347348752\n",
            "Jaccard Score: 0.14644358794674142\n",
            "Jaccard Score: 0.15005839757066106\n",
            "Jaccard Score: 0.1634314412520439\n",
            "Jaccard Score: 0.17515183368371873\n",
            "Jaccard Score: 0.18985050221910765\n",
            "Jaccard Score: 0.20491707544966126\n",
            "Jaccard Score: 0.21127073113758468\n",
            "Jaccard Score: 0.21741999532819434\n",
            "Jaccard Score: 0.21152768044849335\n",
            "Jaccard Score: 0.2102254146227517\n",
            "Jaccard Score: 0.22013548236393365\n",
            "Jaccard Score: 0.22053258584442886\n",
            "Jaccard Score: 0.22585260453165146\n",
            "Jaccard Score: 0.23258000467180565\n",
            "Jaccard Score: 0.23524293389395\n",
            "Jaccard Score: 0.2290644709180098\n",
            "Jaccard Score: 0.22566573230553608\n",
            "Jaccard Score: 0.21500233590282644\n",
            "Jaccard Score: 0.21417309039943938\n",
            "Jaccard Score: 0.2122576500817566\n",
            "Jaccard Score: 0.21712216771782292\n",
            "Jaccard Score: 0.2233940668068208\n",
            "Jaccard Score: 0.23259168418593784\n",
            "Jaccard Score: 0.2355057229619248\n",
            "Jaccard Score: 0.24155571128241066\n",
            "Jaccard Score: 0.23439616911936464\n",
            "Jaccard Score: 0.24399672973604297\n",
            "Jaccard Score: 0.25082924550338703\n",
            "Jaccard Score: 0.24842910534921747\n",
            "Jaccard Score: 0.25701938799345947\n",
            "Jaccard Score: 0.2594020088764307\n",
            "Jaccard Score: 0.2678696566222845\n",
            "Jaccard Score: 0.26867554309740715\n",
            "Jaccard Score: 0.2715603830880635\n",
            "Jaccard Score: 0.2758234057463209\n",
            "Jaccard Score: 0.2813069376313945\n",
            "Jaccard Score: 0.2783987386124737\n",
            "Jaccard Score: 0.2743517869656622\n",
            "Jaccard Score: 0.27991123569259513\n",
            "Jaccard Score: 0.2836136416725064\n",
            "Jaccard Score: 0.2895526746087363\n",
            "Jaccard Score: 0.2881160943704742\n",
            "Jaccard Score: 0.28870007007708476\n",
            "Jaccard Score: 0.28860663396402714\n",
            "Jaccard Score: 0.28538308806353657\n",
            "Jaccard Score: 0.28565171688857743\n",
            "Jaccard Score: 0.2877540294323756\n",
            "Jaccard Score: 0.2843669703340341\n",
            "Jaccard Score: 0.28465895818733944\n",
            "Jaccard Score: 0.27960756832515765\n",
            "Jaccard Score: 0.2841625788367204\n",
            "Jaccard Score: 0.28095655220742816\n",
            "Jaccard Score: 0.2804192945573464\n",
            "Jaccard Score: 0.28672039243167485\n",
            "Jaccard Score: 0.284857509927587\n",
            "Jaccard Score: 0.2882562485400607\n",
            "Jaccard Score: 0.287281009110021\n",
            "Jaccard Score: 0.2854590049053959\n",
            "Jaccard Score: 0.2919119364634431\n",
            "Jaccard Score: 0.28780074748890444\n",
            "Jaccard Score: 0.2861130576967998\n",
            "Jaccard Score: 0.29188273767811257\n",
            "Jaccard Score: 0.29839406680682085\n",
            "Jaccard Score: 0.29905395935529083\n",
            "Jaccard Score: 0.30174024760569956\n",
            "Jaccard Score: 0.30528498014482597\n",
            "Jaccard Score: 0.306996028965195\n",
            "Jaccard Score: 0.30679747722494743\n",
            "Jaccard Score: 0.31286498481663166\n",
            "Jaccard Score: 0.31093202522775054\n",
            "Jaccard Score: 0.31339640270964725\n",
            "Jaccard Score: 0.31427820602662926\n",
            "Jaccard Score: 0.31309857509927586\n",
            "Jaccard Score: 0.323510861948143\n",
            "Jaccard Score: 0.3172740014015417\n",
            "Jaccard Score: 0.3147862648913805\n",
            "Jaccard Score: 0.3126547535622518\n",
            "Jaccard Score: 0.30304251343144123\n",
            "Jaccard Score: 0.2967414155571128\n",
            "Jaccard Score: 0.2884606400373744\n",
            "Jaccard Score: 0.28670871291754263\n",
            "Jaccard Score: 0.28606049988320487\n",
            "Jaccard Score: 0.29215720626021957\n",
            "Jaccard Score: 0.2955092268161644\n",
            "Jaccard Score: 0.2982714319084326\n",
            "Jaccard Score: 0.3007883672039243\n",
            "Jaccard Score: 0.3029081990189208\n",
            "Jaccard Score: 0.29807871992525115\n",
            "Jaccard Score: 0.29863933660359726\n",
            "Jaccard Score: 0.2950887643074048\n",
            "Jaccard Score: 0.29824223312310205\n",
            "Jaccard Score: 0.30120298995561784\n",
            "Jaccard Score: 0.3\n",
            "Jaccard Score: 0.29950362064938096\n",
            "Jaccard Score: 0.3073288951179631\n",
            "Jaccard Score: 0.31254379817799577\n",
            "Jaccard Score: 0.3153410418126606\n",
            "Jaccard Score: 0.31368839056295256\n",
            "Jaccard Score: 0.31732071945807055\n",
            "Jaccard Score: 0.3159074982480729\n",
            "Jaccard Score: 0.3132387292688624\n",
            "Jaccard Score: 0.30024526979677646\n",
            "Jaccard Score: 0.30315346881569727\n",
            "Jaccard Score: 0.3041170287316048\n",
            "Jaccard Score: 0.3004613408082224\n",
            "Jaccard Score: 0.29745970567624386\n",
            "Jaccard Score: 0.3023359028264424\n",
            "Jaccard Score: 0.30864284045783696\n",
            "Jaccard Score: 0.29877949077318383\n",
            "Jaccard Score: 0.3041812660593319\n",
            "Jaccard Score: 0.3039593552908199\n",
            "Jaccard Score: 0.30578719925251113\n",
            "Jaccard Score: 0.30073580939032935\n",
            "Jaccard Score: 0.3016409717355758\n",
            "Jaccard Score: 0.3041695865451997\n",
            "Jaccard Score: 0.30871291754263025\n",
            "Jaccard Score: 0.32056178462975937\n",
            "Jaccard Score: 0.3261153935996262\n",
            "Jaccard Score: 0.32317215603830884\n",
            "Jaccard Score: 0.3247196916608269\n",
            "Jaccard Score: 0.32249474421864044\n",
            "Jaccard Score: 0.31207661761270733\n",
            "Jaccard Score: 0.31028965195047886\n",
            "Jaccard Score: 0.3098399906563887\n",
            "Jaccard Score: 0.30999766409717355\n",
            "Jaccard Score: 0.3086078019154403\n",
            "Jaccard Score: 0.3094545666900257\n",
            "Jaccard Score: 0.312228451296426\n",
            "Jaccard Score: 0.31278906797477224\n",
            "Jaccard Score: 0.3104531651483298\n",
            "Jaccard Score: 0.3148738612473721\n",
            "Jaccard Score: 0.32054426535856106\n",
            "Jaccard Score: 0.322354590049054\n",
            "Jaccard Score: 0.32759285213735106\n",
            "Jaccard Score: 0.3350911002102312\n",
            "Jaccard Score: 0.3314879701004438\n",
            "Jaccard Score: 0.3362006540527914\n",
            "Jaccard Score: 0.3403819201121233\n",
            "Jaccard Score: 0.3319609904227984\n",
            "Jaccard Score: 0.33865335202055596\n",
            "Jaccard Score: 0.33567507591684187\n",
            "Jaccard Score: 0.3280775519738379\n",
            "Jaccard Score: 0.32697383788834383\n",
            "Jaccard Score: 0.32620882971268395\n",
            "Jaccard Score: 0.3243167484232656\n",
            "Jaccard Score: 0.32506423732772716\n",
            "Jaccard Score: 0.3231020789535155\n",
            "Jaccard Score: 0.3164681149264191\n",
            "Jaccard Score: 0.3187105816398038\n",
            "Jaccard Score: 0.32073697734174256\n",
            "Jaccard Score: 0.32659425367904693\n",
            "Jaccard Score: 0.3257825274468582\n",
            "Jaccard Score: 0.3286031301097874\n",
            "Jaccard Score: 0.32440434477925717\n",
            "Jaccard Score: 0.32605699602896515\n",
            "Jaccard Score: 0.3232889511796309\n",
            "Jaccard Score: 0.3280133146461107\n",
            "Jaccard Score: 0.32731838355524406\n",
            "Jaccard Score: 0.32774468582106986\n",
            "Jaccard Score: 0.32929222144358794\n",
            "Jaccard Score: 0.3225064237327727\n",
            "Jaccard Score: 0.317402476056996\n",
            "Jaccard Score: 0.3202230787199252\n",
            "Jaccard Score: 0.3170228918476991\n",
            "Jaccard Score: 0.31399205793039014\n",
            "Jaccard Score: 0.3129000233590283\n",
            "Jaccard Score: 0.3056645643541228\n",
            "Jaccard Score: 0.3142139686989021\n",
            "Jaccard Score: 0.31411469282877835\n",
            "Jaccard Score: 0.3137000700770848\n",
            "Jaccard Score: 0.31786381686521836\n",
            "Jaccard Score: 0.32284512964260687\n",
            "Jaccard Score: 0.3183017986451763\n",
            "Jaccard Score: 0.324229152067274\n",
            "Jaccard Score: 0.3237794907731838\n",
            "Jaccard Score: 0.32911118897453867\n",
            "Jaccard Score: 0.32919878533053026\n",
            "Jaccard Score: 0.32969516468114923\n",
            "Jaccard Score: 0.3290060733473487\n",
            "Jaccard Score: 0.3279899556178463\n",
            "Jaccard Score: 0.32949661294090166\n",
            "Jaccard Score: 0.32165381920112124\n",
            "Jaccard Score: 0.31795141322120996\n",
            "Jaccard Score: 0.3205676243868255\n",
            "Jaccard Score: 0.32186405045550104\n",
            "Jaccard Score: 0.3203281943471152\n",
            "Jaccard Score: 0.32448026162111654\n",
            "Jaccard Score: 0.3250058397570661\n",
            "Jaccard Score: 0.3266818500350385\n",
            "Jaccard Score: 0.32648329829479095\n",
            "Jaccard Score: 0.33178579771081523\n",
            "Jaccard Score: 0.33278439616911937\n",
            "Jaccard Score: 0.3304134548002803\n",
            "Jaccard Score: 0.33380635365568795\n",
            "Jaccard Score: 0.3346180798878767\n",
            "Jaccard Score: 0.333975706610605\n",
            "Jaccard Score: 0.3365393599626256\n",
            "Jaccard Score: 0.34201705209063304\n",
            "Jaccard Score: 0.3398446624620416\n",
            "Jaccard Score: 0.32880752160710114\n",
            "Jaccard Score: 0.3260044382153702\n",
            "Jaccard Score: 0.3266409717355758\n",
            "Jaccard Score: 0.31992525110955383\n",
            "Jaccard Score: 0.3177061434244335\n",
            "Jaccard Score: 0.31428404578369534\n",
            "Jaccard Score: 0.3208304134548003\n",
            "Jaccard Score: 0.3220918009810792\n",
            "Jaccard Score: 0.3183601962158374\n",
            "Jaccard Score: 0.3166783461807989\n",
            "Jaccard Score: 0.32091217005372574\n",
            "Jaccard Score: 0.32150198551740244\n",
            "Jaccard Score: 0.32168885774351785\n",
            "Jaccard Score: 0.32368021490306004\n",
            "Jaccard Score: 0.3250934361130577\n",
            "Jaccard Score: 0.3188565755664564\n",
            "Jaccard Score: 0.3143891614108853\n",
            "Jaccard Score: 0.31113641672506426\n",
            "Jaccard Score: 0.30604414856341977\n",
            "Jaccard Score: 0.30196215837421164\n",
            "Jaccard Score: 0.3032235459004905\n",
            "Jaccard Score: 0.30356225181032465\n",
            "Jaccard Score: 0.3133438448960523\n",
            "Jaccard Score: 0.31219341275402945\n",
            "Jaccard Score: 0.31904928754963796\n",
            "Jaccard Score: 0.32368021490306004\n",
            "Jaccard Score: 0.3281009110021023\n",
            "Jaccard Score: 0.3323989722027563\n",
            "Jaccard Score: 0.3325157673440785\n",
            "Jaccard Score: 0.33781242700303665\n",
            "Jaccard Score: 0.33376547535622514\n",
            "Jaccard Score: 0.3296017285680916\n",
            "Jaccard Score: 0.3349801448259752\n",
            "Jaccard Score: 0.33296542863816864\n",
            "Jaccard Score: 0.33763723429105347\n",
            "Jaccard Score: 0.3316047652417659\n",
            "Jaccard Score: 0.33074048119598226\n",
            "Jaccard Score: 0.3262380284980145\n",
            "Jaccard Score: 0.3278848399906564\n",
            "Jaccard Score: 0.31762438682550803\n",
            "Jaccard Score: 0.31282410651716885\n",
            "Jaccard Score: 0.31381686521840696\n",
            "Jaccard Score: 0.3140212567157206\n",
            "Jaccard Score: 0.314371642139687\n",
            "Jaccard Score: 0.3114459238495678\n",
            "Jaccard Score: 0.3098283111422565\n",
            "Jaccard Score: 0.31465779023592616\n",
            "Jaccard Score: 0.3184594720859612\n",
            "Jaccard Score: 0.318336837187573\n",
            "Jaccard Score: 0.32439266526512495\n",
            "Jaccard Score: 0.3247196916608269\n",
            "Jaccard Score: 0.33390562952581176\n",
            "Jaccard Score: 0.3291579070310675\n",
            "Jaccard Score: 0.3334968465311843\n",
            "Jaccard Score: 0.3344604064470918\n",
            "Jaccard Score: 0.3316514832982948\n",
            "Jaccard Score: 0.33140621350151833\n",
            "Jaccard Score: 0.33495094604064474\n",
            "Jaccard Score: 0.3350911002102312\n",
            "Jaccard Score: 0.3373861247372109\n",
            "Epoch   274: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Jaccard Score: 0.34133964027096475\n",
            "Jaccard Score: 0.33102662929222143\n",
            "Jaccard Score: 0.3323697734174258\n",
            "Jaccard Score: 0.33076384022424665\n",
            "Jaccard Score: 0.33393482831114224\n",
            "Jaccard Score: 0.32989955617846295\n",
            "Jaccard Score: 0.3336895585143658\n",
            "Jaccard Score: 0.32926886241532355\n",
            "Jaccard Score: 0.33551156271899085\n",
            "Jaccard Score: 0.33481079187105817\n",
            "Jaccard Score: 0.33607801915440316\n",
            "Jaccard Score: 0.3304309740714786\n",
            "Jaccard Score: 0.3329245503387059\n",
            "Jaccard Score: 0.3306003270263957\n",
            "Jaccard Score: 0.3346122401308106\n",
            "Jaccard Score: 0.33691310441485633\n",
            "Jaccard Score: 0.337059098341509\n",
            "Jaccard Score: 0.33155804718523707\n",
            "Jaccard Score: 0.3316690025694931\n",
            "Jaccard Score: 0.3371817332398972\n",
            "Jaccard Score: 0.3326967998131278\n",
            "Jaccard Score: 0.33686054660126136\n",
            "Jaccard Score: 0.3316398037841626\n",
            "Jaccard Score: 0.3328252744685821\n",
            "Jaccard Score: 0.3342910534921747\n",
            "Jaccard Score: 0.33510861948142956\n",
            "Jaccard Score: 0.33684302733006305\n",
            "Jaccard Score: 0.33712333566923613\n",
            "Jaccard Score: 0.3381277738846064\n",
            "Jaccard Score: 0.3346998364868021\n",
            "Jaccard Score: 0.33545316514832985\n",
            "Jaccard Score: 0.33642840457836953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-c0e344c69cdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mall_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdev_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m       \u001b[0mall_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mconcat_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-03df86f9e067>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1922\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WNBFOGtJ7zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = lambda x: P(F(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFkYxjMrRPNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_testfile = \"/content/2018-E-c-En-test-gold.txt\"\n",
        "\n",
        "en_testdata = file_to_data(en_testfile)\n",
        "en_test_tweets = get_all_tweets(en_testdata)\n",
        "en_test_embeddings = laser.embed_sentences(en_test_tweets, lang='en')\n",
        "\n",
        "test_label_tensors = get_label_tensors(en_testdata)\n",
        "y_true = torch.stack(test_label_tensors)\n",
        "\n",
        "en_test_dataset = TensorDataset(torch.from_numpy(en_test_embeddings), y_true)\n",
        "test_dataloader = DataLoader(en_test_dataset, batch_size=100, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfelwQOuJ80u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "outputId": "83f9ae6d-f9a5-49ed-a7e0-3de63e2dd86d"
      },
      "source": [
        "THRESHOLD = 0.5\n",
        "up = 0\n",
        "\n",
        "all_list = []\n",
        "\n",
        "for x, y in test_dataloader:\n",
        "    guess = net(x)\n",
        "    all_list.append(guess)\n",
        "    # print(guess)\n",
        "    # values, indices = torch.topk(guess, 2)\n",
        "    # print(\"values:\", values)\n",
        "    # print(\"indices:\", indices)\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(11):\n",
        "            if (guess[i][j] >= THRESHOLD and y[i][j] == 1) or (guess[i][j] < THRESHOLD and y[i][j] == 0):\n",
        "              up += 1\n",
        "\n",
        "concat_tensor = torch.cat(all_list)\n",
        "y_score = concat_tensor.detach().numpy()\n",
        "print(concat_tensor)\n",
        "\n",
        "\n",
        "print(\"ACCURACY:\", up / (len(test_label_tensors)*11))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[  0.7187,  -4.6658,   1.9812,  ...,  -1.1565,  -7.7492,  -9.0842],\n",
            "        [  0.4551,  -2.1471,   0.6134,  ...,  -1.7455,  -4.0128,  -5.4623],\n",
            "        [  3.7854,  -4.7268,   2.1217,  ...,  -0.5551,  -6.8569,  -7.2609],\n",
            "        ...,\n",
            "        [  8.9609, -10.3167,   5.1475,  ...,  -0.9895, -10.8832, -16.0266],\n",
            "        [ -2.7824,  -1.2962,  -2.0994,  ...,   1.0403,  -2.7024,  -3.4429],\n",
            "        [  1.7984,  -2.9886,   0.9447,  ...,  -2.3721,  -3.2036,  -6.7177]],\n",
            "       grad_fn=<CatBackward>)\n",
            "ACCURACY: 0.8424781723339563\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YgBbcHNZCGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1rVEs_zR__e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ce8eb66-7450-49bc-c217-85c06354bdda"
      },
      "source": [
        "lrap_score = label_ranking_average_precision_score(y_true, y_score)\n",
        "print(\"LRAP Score:\", lrap_score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LRAP Score: 0.7651428614968053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5R5ZNFZBBIh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "e830db7e-d266-4588-fa31-8e34e67a0e6c"
      },
      "source": [
        "from sklearn.metrics import f1_score, jaccard_score\n",
        "y_pred = y_score.copy()\n",
        "for i in range(y_pred.shape[0]):\n",
        "  for j in range(y_pred.shape[1]):\n",
        "    if (y_pred[i][j] >= 0.5):\n",
        "      y_pred[i][j] = 1\n",
        "    else:\n",
        "      y_pred[i][j] = 0\n",
        "f1score = f1_score(y_true, y_pred, average=None)\n",
        "print(\"F1 Score:\", f1score)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "print(\"F1 Micro Score:\", f1_micro)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "print(\"F1 Macro Score:\", f1_macro)\n",
        "jac = jaccard_score(y_true, y_pred, average='samples')\n",
        "print(\"Jaccard Score:\", jac)\n",
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 Score: [0.66603325 0.12252964 0.62851505 0.56828194 0.72594397 0.34361233\n",
            " 0.55264624 0.13995485 0.56850962 0.06521739 0.03636364]\n",
            "F1 Micro Score: 0.5636349586585272\n",
            "F1 Macro Score: 0.4016007204607273\n",
            "Jaccard Score: 0.4356587618170926\n",
            "[[1. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 1. ... 0. 0. 0.]\n",
            " [1. 0. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [1. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 1. 0. 0.]\n",
            " [1. 0. 1. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxL0t_TTeb6a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5c0cf8fd-5845-4b67-e1be-ddfcb448f84f"
      },
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(hl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.1603113057546933\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVC7lU-j3UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(P.state_dict(), '/content/P_en_0712.model')\n",
        "torch.save(F.state_dict(), '/content/F_en_0712.model')\n",
        "torch.save(Q.state_dict(), '/content/Q_en_0712.model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spanish to English",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOijY+ba8KAsL/RRNxE7STP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jigsawfallingintoplace/KAUST/blob/master/Spanish_to_English.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RDWh1g__qFy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install laserembeddings\n",
        "!pip install ekphrasis\n",
        "!pip install emoji"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyhbMzuuhrat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
        "from ekphrasis.classes.tokenizer import SocialTokenizer\n",
        "from ekphrasis.dicts.emoticons import emoticons\n",
        "\n",
        "omit_list = ['url', 'email', 'phone', 'user']\n",
        "\n",
        "text_processor = TextPreProcessor(\n",
        "    # omit=['url', 'email', 'phone', 'user'],\n",
        "    # terms that will be normalized\n",
        "    normalize=omit_list,\n",
        "    # terms that will be annotated\n",
        "    # annotate={\"hashtag\", \"allcaps\", \"elongated\", \"repeated\",\n",
        "    #     'emphasis', 'censored'},\n",
        "    fix_html=True,  # fix HTML tokens\n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for word segmentation \n",
        "    segmenter=\"twitter\", \n",
        "    \n",
        "    # corpus from which the word statistics are going to be used \n",
        "    # for spell correction\n",
        "    corrector=\"twitter\", \n",
        "    \n",
        "    unpack_hashtags=True,  # perform word segmentation on hashtags\n",
        "    unpack_contractions=False,  # Unpack contractions (can't -> can not)\n",
        "    spell_correct_elong=False,  # spell correction for elongated words\n",
        "    \n",
        "    # select a tokenizer. You can use SocialTokenizer, or pass your own\n",
        "    # the tokenizer, should take as input a string and return a list of tokens\n",
        "    tokenizer=lambda s: s.split(),\n",
        "    \n",
        "    # list of dictionaries, for replacing tokens extracted from the text,\n",
        "    # with other expressions. You can pass more than one dictionaries.\n",
        "    dicts=[emoticons]\n",
        ")\n",
        "\n",
        "sentences = [\n",
        "    \"CANT WAIT for the new season of #TwinPeaks ＼(^o^)／!!! #davidlynch #tvseries :)))\",\n",
        "    \"I saw the new #johndoe movie and it suuuuucks!!! WAISTED $10... #badmovies :/\",\n",
        "    \"@SentimentSymp:  can't wait for the Nov 9 #Sentiment talks!  YAAAAAAY !!! :-D http://sentimentsymposium.com/.\"\n",
        "]\n",
        "\n",
        "for s in sentences:\n",
        "    print(\" \".join(text_processor.pre_process_doc(s)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AzmXOe5_66p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c99d31be-f996-4186-9c47-6c4db2aa07b9"
      },
      "source": [
        "!python -m laserembeddings download-models"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading models into /usr/local/lib/python3.6/dist-packages/laserembeddings/data\n",
            "\n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fcodes    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/93langs.fvocab    \n",
            "✅   Downloaded https://dl.fbaipublicfiles.com/laser/models/bilstm.93langs.2018-12-26.pt    \n",
            "\n",
            "✨ You're all set!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkR3fa21_-12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import emoji\n",
        "from laserembeddings import Laser\n",
        "import torch\n",
        "from torch import optim, nn\n",
        "\n",
        "laser = Laser()\n",
        "\n",
        "en_trainfile = \"/content/2018-E-c-En-train.txt\"\n",
        "en_devfile = \"/content/2018-E-c-En-dev.txt\"\n",
        "en_testfile = \"/content/2018-E-c-En-test-gold.txt\"\n",
        "\n",
        "es_trainfile = \"/content/2018-E-c-Es-train.txt\"\n",
        "es_testfile = \"/content/2018-E-c-Es-test-gold.txt\"\n",
        "es_devfile = \"/content/2018-E-c-Es-dev.txt\"\n",
        "savepath = \"/content/LASERSentiment.model\"\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def file_to_data(file):\n",
        "    with open(file) as f:\n",
        "        reader = csv.reader(f, delimiter=\"\\t\")\n",
        "        data = list(reader)\n",
        "    return data\n",
        "\n",
        "traindata = file_to_data(en_trainfile)\n",
        "devdata = file_to_data(es_devfile)\n",
        "testdata = file_to_data(es_testfile)\n",
        "en_testdata = file_to_data(en_testfile)\n",
        "\n",
        "def process(s):\n",
        "  ret = \" \".join(text_processor.pre_process_doc(emoji.demojize(s)))\n",
        "  for item in omit_list:\n",
        "    ret.replace(\"<\" + item + \">\", '')\n",
        "  return ret\n",
        "\n",
        "def get_all_tweets(data): \n",
        "    return [process(d[1]) for d in data[1:]]\n",
        "\n",
        "def get_label_lists(data):\n",
        "    return [[int(x) for x in d[2:]] for d in data[1:]]\n",
        "\n",
        "def get_label_tensors(data):\n",
        "    label_tensors = []\n",
        "    for d in data[1:]:\n",
        "        tmp = torch.zeros(11)\n",
        "        for i in range(11):\n",
        "            if d[2 + i] == '1':\n",
        "                tmp[i] = 1\n",
        "        label_tensors.append(tmp)\n",
        "    return label_tensors\n",
        "\n",
        "train_tweets = get_all_tweets(traindata)\n",
        "train_embeddings = laser.embed_sentences(train_tweets, lang='en')\n",
        "dev_tweets = get_all_tweets(devdata)\n",
        "dev_embeddings = laser.embed_sentences(dev_tweets, lang='es')\n",
        "test_tweets = get_all_tweets(testdata)\n",
        "test_embeddings = laser.embed_sentences(test_tweets, lang='es')"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJdzs78NZ0Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "en_test_tweets = get_all_tweets(en_testdata)\n",
        "en_test_embeddings = laser.embed_sentences(en_test_tweets, lang='en')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCGmzqRQiDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        self.layer_1 = nn.Linear(1024, 512)\n",
        "        self.layer_2 = nn.Linear(512, 512)\n",
        "        self.layer_3 = nn.Linear(512, 512)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm1 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm2 = nn.BatchNorm1d(512)\n",
        "        self.batchnorm3 = nn.BatchNorm1d(512)\n",
        "    def forward(self, x):\n",
        "        x = self.layer_1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.relu(x)\n",
        "        # x = self.dropout(x)\n",
        "        return x"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3zWGvug_YWlg",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier, self).__init__()\n",
        "        self.layer_4 = nn.Linear(512, 512)\n",
        "        self.layer_out = nn.Linear(512, 11) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(512)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        x = self.layer_4(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_out(x)\n",
        "        # x = self.sigmoid(x)\n",
        "        return x"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F7H91PO8YX7O",
        "colab": {}
      },
      "source": [
        "class LanguageClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LanguageClassifier, self).__init__()\n",
        "        self.layer_4 = nn.Linear(512, 512)\n",
        "        self.layer_out = nn.Linear(512, 1) \n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.batchnorm4 = nn.BatchNorm1d(512)\n",
        "    def forward(self, x):\n",
        "        x = self.layer_4(x)\n",
        "        x = self.batchnorm4(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        x = self.layer_out(x)\n",
        "        # x = nn.Softmax(dim=1)(x)\n",
        "        return x"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xmRErUuUNs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_embeddings_tensors = torch.from_numpy(train_embeddings)\n",
        "label_tensors = get_label_tensors(traindata)\n",
        "assert len(train_embeddings_tensors) == len(label_tensors)\n",
        "\n",
        "train_dataset = TensorDataset(train_embeddings_tensors, torch.stack(label_tensors))\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=100, shuffle=True)\n",
        "train_iter_source = iter(train_dataloader)\n",
        "\n",
        "\n",
        "test_label_tensors = get_label_tensors(testdata)\n",
        "y_true = torch.stack(test_label_tensors)\n",
        "\n",
        "test_dataset = TensorDataset(torch.from_numpy(test_embeddings), y_true)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False)\n",
        "\n",
        "train_iter_target = iter(test_dataloader)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MCPR6A1aZWS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_function = nn.BCEWithLogitsLoss()\n",
        "lambd = 0.01\n",
        "learning_rate = 0.005\n",
        "Q_learning_rate = 0.005\n",
        "F = FeatureExtractor()\n",
        "P = SentimentClassifier()\n",
        "Q = LanguageClassifier()\n",
        "F, P, Q = F.to(device), P.to(device), Q.to(device)\n",
        "optimizer = optim.Adam(list(F.parameters()) + list(P.parameters()), lr=learning_rate)\n",
        "q_optimizer = optim.Adam(Q.parameters(), lr=Q_learning_rate)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5e1ce3Wso016",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# net.load_state_dict(torch.load(savepath))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYz-KclCWufJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def freeze(net):\n",
        "    for p in net.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def unfreeze(net):\n",
        "    for p in net.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def get_batch_source():\n",
        "  global train_iter_source\n",
        "  try:\n",
        "    return next(train_iter_source)\n",
        "  except:\n",
        "    train_iter_source = iter(train_dataloader)\n",
        "    return next(train_iter_source)\n",
        "\n",
        "def get_batch_target():\n",
        "  global train_iter_target\n",
        "  try:\n",
        "    return next(train_iter_target)\n",
        "  except:\n",
        "    train_iter_target = iter(test_dataloader)\n",
        "    return next(train_iter_target)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5XrSnHjWIIb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b88eda7-2ae4-4e3c-8b75-73180f8c5cff"
      },
      "source": [
        "from sklearn.metrics import label_ranking_average_precision_score, f1_score, jaccard_score, hamming_loss\n",
        "\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=60, verbose=True)\n",
        "\n",
        "\n",
        "dev_label_tensors = get_label_tensors(devdata)\n",
        "dev_y_true = torch.stack(dev_label_tensors)\n",
        "up = 0\n",
        "\n",
        "dev_dataset = TensorDataset(torch.from_numpy(dev_embeddings), dev_y_true)\n",
        "dev_dataloader = DataLoader(dev_dataset, batch_size=10, shuffle=False)\n",
        "\n",
        "loss_diagram = []\n",
        "num_epochs = 100\n",
        "q_iter = 5\n",
        "cnt = 0\n",
        "clip_lower = -0.01\n",
        "clip_upper = 0.01\n",
        "running_loss = 0\n",
        "jac = 0\n",
        "while (jac < 0.45):\n",
        "    freeze(P)\n",
        "    freeze(F)\n",
        "    unfreeze(Q)\n",
        "    for q in range(q_iter):\n",
        "      for p in Q.parameters():\n",
        "        p.data.clamp_(clip_lower, clip_upper)\n",
        "      \n",
        "      Q.zero_grad()\n",
        "\n",
        "      X_source, _ = get_batch_source()\n",
        "      X_target, _ = get_batch_target()\n",
        "      feature_source = F(X_source)\n",
        "      feature_target = F(X_target)\n",
        "      loss_q = torch.mean(-Q(feature_source)) + torch.mean(Q(feature_target))\n",
        "      q_optimizer.zero_grad()\n",
        "      loss_q.backward()\n",
        "      q_optimizer.step()\n",
        "\n",
        "    unfreeze(F)\n",
        "    unfreeze(P)\n",
        "    freeze(Q)\n",
        "\n",
        "    for p in Q.parameters():\n",
        "        p.data.clamp_(clip_lower, clip_upper) \n",
        "\n",
        "    F.zero_grad()\n",
        "    P.zero_grad()\n",
        "\n",
        "    X_source, Y_source = get_batch_source()\n",
        "    X_target, Y_target = get_batch_target()\n",
        "    feature_source = F(X_source)\n",
        "    sentiment_source = P(feature_source)\n",
        "    language_source = Q(feature_source)\n",
        "\n",
        "    feature_target = F(X_target)\n",
        "    language_target = Q(feature_target)\n",
        "\n",
        "    loss_sentiment = loss_function(sentiment_source, Y_source)\n",
        "    loss_sentiment.backward(retain_graph=True)\n",
        "    loss_language = lambd*(torch.mean(language_source) - torch.mean(language_target))\n",
        "    loss_language.backward(retain_graph=True)\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "    all_list = []\n",
        "    for x, y in dev_dataloader:\n",
        "      guess = P(F(x))\n",
        "      all_list.append(guess)\n",
        "    concat_tensor = torch.cat(all_list)\n",
        "    dev_y_score = concat_tensor.detach().numpy()\n",
        "    dev_y_pred = dev_y_score.copy()\n",
        "    for i in range(dev_y_pred.shape[0]):\n",
        "      for j in range(dev_y_pred.shape[1]):\n",
        "        if (dev_y_pred[i][j] >= 0.5):\n",
        "          dev_y_pred[i][j] = 1\n",
        "        else:\n",
        "          dev_y_pred[i][j] = 0\n",
        "    jac = jaccard_score(dev_y_true, dev_y_pred, average='samples')\n",
        "    print(\"Jaccard Score:\", jac)\n",
        "    scheduler.step(jac)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Jaccard is ill-defined and being set to 0.0 in samples with no true or predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Jaccard Score: 0.010309278350515464\n",
            "Jaccard Score: 0.05608738340697103\n",
            "Jaccard Score: 0.10088365243004419\n",
            "Jaccard Score: 0.13193421698576338\n",
            "Jaccard Score: 0.14727540500736377\n",
            "Jaccard Score: 0.16512027491408937\n",
            "Jaccard Score: 0.16666666666666666\n",
            "Jaccard Score: 0.18544428080510553\n",
            "Jaccard Score: 0.20189003436426115\n",
            "Jaccard Score: 0.22140402552773683\n",
            "Jaccard Score: 0.25184094256259204\n",
            "Jaccard Score: 0.24975454099165437\n",
            "Jaccard Score: 0.240672557682867\n",
            "Jaccard Score: 0.2373588610702013\n",
            "Jaccard Score: 0.22022582228767795\n",
            "Jaccard Score: 0.2068483063328424\n",
            "Jaccard Score: 0.19653902798232695\n",
            "Jaccard Score: 0.1990427098674521\n",
            "Jaccard Score: 0.1940844378988709\n",
            "Jaccard Score: 0.19592538046146293\n",
            "Jaccard Score: 0.1947717231222386\n",
            "Jaccard Score: 0.20851742758959252\n",
            "Jaccard Score: 0.21961217476681397\n",
            "Jaccard Score: 0.22476681394207168\n",
            "Jaccard Score: 0.23649975454099167\n",
            "Jaccard Score: 0.2540500736377025\n",
            "Jaccard Score: 0.2601865488463426\n",
            "Jaccard Score: 0.2601865488463426\n",
            "Jaccard Score: 0.2706185567010309\n",
            "Jaccard Score: 0.27601865488463423\n",
            "Jaccard Score: 0.2739813451153657\n",
            "Jaccard Score: 0.2778595974472263\n",
            "Jaccard Score: 0.27626411389297983\n",
            "Jaccard Score: 0.2778350515463917\n",
            "Jaccard Score: 0.2707903780068729\n",
            "Jaccard Score: 0.27557682866961214\n",
            "Jaccard Score: 0.27096219931271476\n",
            "Jaccard Score: 0.2863279332351497\n",
            "Jaccard Score: 0.2849042709867452\n",
            "Jaccard Score: 0.2944035346097202\n",
            "Jaccard Score: 0.2961953853706431\n",
            "Jaccard Score: 0.3000490918016691\n",
            "Jaccard Score: 0.30930289641629843\n",
            "Jaccard Score: 0.29548355424644085\n",
            "Jaccard Score: 0.3116593028964163\n",
            "Jaccard Score: 0.31151202749140894\n",
            "Jaccard Score: 0.3015954835542464\n",
            "Jaccard Score: 0.29779086892488954\n",
            "Jaccard Score: 0.2982817869415807\n",
            "Jaccard Score: 0.3077810505645557\n",
            "Jaccard Score: 0.2903043691703485\n",
            "Jaccard Score: 0.2973490427098674\n",
            "Jaccard Score: 0.29874815905743746\n",
            "Jaccard Score: 0.28568973981345114\n",
            "Jaccard Score: 0.2872115856651939\n",
            "Jaccard Score: 0.2936671575846833\n",
            "Jaccard Score: 0.2918016691212567\n",
            "Jaccard Score: 0.2812469317623956\n",
            "Jaccard Score: 0.28532155130093273\n",
            "Jaccard Score: 0.28294059891998036\n",
            "Jaccard Score: 0.2952626411389298\n",
            "Jaccard Score: 0.2815905743740795\n",
            "Jaccard Score: 0.2896170839469809\n",
            "Jaccard Score: 0.2873588610702013\n",
            "Jaccard Score: 0.2997545409916544\n",
            "Jaccard Score: 0.3029945999018164\n",
            "Jaccard Score: 0.3089592538046146\n",
            "Jaccard Score: 0.30569464899361803\n",
            "Jaccard Score: 0.3088856161021109\n",
            "Jaccard Score: 0.3051300932744232\n",
            "Jaccard Score: 0.3220176730486009\n",
            "Jaccard Score: 0.3130093274423171\n",
            "Jaccard Score: 0.31698576337751594\n",
            "Jaccard Score: 0.3123956799214531\n",
            "Jaccard Score: 0.3109229258713795\n",
            "Jaccard Score: 0.3049828178694158\n",
            "Jaccard Score: 0.3001963672066765\n",
            "Jaccard Score: 0.2959990181639666\n",
            "Jaccard Score: 0.28784977908689247\n",
            "Jaccard Score: 0.30164457535591555\n",
            "Jaccard Score: 0.28306332842415316\n",
            "Jaccard Score: 0.2884634266077565\n",
            "Jaccard Score: 0.2817133038782523\n",
            "Jaccard Score: 0.278325969563083\n",
            "Jaccard Score: 0.28902798232695137\n",
            "Jaccard Score: 0.29783996072655866\n",
            "Jaccard Score: 0.2957290132547864\n",
            "Jaccard Score: 0.3112665684830633\n",
            "Jaccard Score: 0.30918016691212563\n",
            "Jaccard Score: 0.3118065783014236\n",
            "Jaccard Score: 0.3162739322533137\n",
            "Jaccard Score: 0.3189494354442808\n",
            "Jaccard Score: 0.3241531664212077\n",
            "Jaccard Score: 0.32250859106529206\n",
            "Jaccard Score: 0.33492881688757975\n",
            "Jaccard Score: 0.32778595974472263\n",
            "Jaccard Score: 0.33522336769759453\n",
            "Jaccard Score: 0.33797250859106526\n",
            "Jaccard Score: 0.3360824742268041\n",
            "Jaccard Score: 0.31914580265095727\n",
            "Jaccard Score: 0.33497790868924887\n",
            "Jaccard Score: 0.3234413352970054\n",
            "Jaccard Score: 0.32506136475208636\n",
            "Jaccard Score: 0.3196858124693176\n",
            "Jaccard Score: 0.32304860088365245\n",
            "Jaccard Score: 0.3140648011782032\n",
            "Jaccard Score: 0.3243740795287187\n",
            "Jaccard Score: 0.31097201767304855\n",
            "Jaccard Score: 0.31197839960726553\n",
            "Jaccard Score: 0.311340206185567\n",
            "Jaccard Score: 0.31759941089837995\n",
            "Jaccard Score: 0.32304860088365245\n",
            "Jaccard Score: 0.3310505645557192\n",
            "Jaccard Score: 0.34327442317133033\n",
            "Jaccard Score: 0.3364997545409916\n",
            "Jaccard Score: 0.32790868924889544\n",
            "Jaccard Score: 0.32970054000981835\n",
            "Jaccard Score: 0.32736867943053505\n",
            "Jaccard Score: 0.32258222876779574\n",
            "Jaccard Score: 0.32503681885125185\n",
            "Jaccard Score: 0.3283505154639175\n",
            "Jaccard Score: 0.3262641138929798\n",
            "Jaccard Score: 0.32839960726558665\n",
            "Jaccard Score: 0.3234904270986745\n",
            "Jaccard Score: 0.32496318114874817\n",
            "Jaccard Score: 0.33635247913598426\n",
            "Jaccard Score: 0.3377025036818851\n",
            "Jaccard Score: 0.32844869906725577\n",
            "Jaccard Score: 0.3448944526264114\n",
            "Jaccard Score: 0.3456308296514482\n",
            "Jaccard Score: 0.337432498772705\n",
            "Jaccard Score: 0.3365733922434953\n",
            "Jaccard Score: 0.3280068728522337\n",
            "Jaccard Score: 0.3287923416789396\n",
            "Jaccard Score: 0.3378252331860579\n",
            "Jaccard Score: 0.34950908198330877\n",
            "Jaccard Score: 0.34776632302405497\n",
            "Jaccard Score: 0.3443789887088856\n",
            "Jaccard Score: 0.3518900343642612\n",
            "Jaccard Score: 0.3373834069710358\n",
            "Jaccard Score: 0.3316642120765832\n",
            "Jaccard Score: 0.338046146293569\n",
            "Jaccard Score: 0.3385125184094256\n",
            "Jaccard Score: 0.33291605301914584\n",
            "Jaccard Score: 0.3337506136475209\n",
            "Jaccard Score: 0.3415316642120766\n",
            "Jaccard Score: 0.3209131075110456\n",
            "Jaccard Score: 0.31472754050073637\n",
            "Jaccard Score: 0.32852233676975945\n",
            "Jaccard Score: 0.3147029945999018\n",
            "Jaccard Score: 0.3091556210112911\n",
            "Jaccard Score: 0.32152675503190964\n",
            "Jaccard Score: 0.3250859106529209\n",
            "Jaccard Score: 0.33863524791359845\n",
            "Jaccard Score: 0.3381934216985763\n",
            "Jaccard Score: 0.34869906725576827\n",
            "Jaccard Score: 0.3482326951399116\n",
            "Jaccard Score: 0.35267550319096713\n",
            "Jaccard Score: 0.3444526264113893\n",
            "Jaccard Score: 0.34678448699067255\n",
            "Jaccard Score: 0.3422189494354443\n",
            "Jaccard Score: 0.33392243495336277\n",
            "Jaccard Score: 0.32604320078546883\n",
            "Jaccard Score: 0.31597938144329896\n",
            "Jaccard Score: 0.3265095729013255\n",
            "Jaccard Score: 0.31492390770741285\n",
            "Jaccard Score: 0.3229013254786451\n",
            "Jaccard Score: 0.3230486008836524\n",
            "Jaccard Score: 0.32432498772704954\n",
            "Jaccard Score: 0.3272950417280314\n",
            "Jaccard Score: 0.32417771232204223\n",
            "Jaccard Score: 0.3266813942071674\n",
            "Jaccard Score: 0.3299459990181639\n",
            "Jaccard Score: 0.33257241040746194\n",
            "Jaccard Score: 0.3342415316642121\n",
            "Jaccard Score: 0.3232449680903289\n",
            "Jaccard Score: 0.31789396170839473\n",
            "Jaccard Score: 0.3191703485517918\n",
            "Jaccard Score: 0.32989690721649484\n",
            "Jaccard Score: 0.32550319096710845\n",
            "Jaccard Score: 0.3357633775159548\n",
            "Jaccard Score: 0.3338487972508591\n",
            "Jaccard Score: 0.3284977908689249\n",
            "Jaccard Score: 0.3292832596956308\n",
            "Jaccard Score: 0.31578301423662253\n",
            "Jaccard Score: 0.3243495336278841\n",
            "Jaccard Score: 0.31362297496318114\n",
            "Jaccard Score: 0.32032400589101623\n",
            "Jaccard Score: 0.3188757977417771\n",
            "Jaccard Score: 0.3139175257731958\n",
            "Jaccard Score: 0.3077565046637211\n",
            "Jaccard Score: 0.30999018163966613\n",
            "Jaccard Score: 0.31698576337751594\n",
            "Jaccard Score: 0.3024791359842906\n",
            "Jaccard Score: 0.31269023073146784\n",
            "Jaccard Score: 0.33063328424153166\n",
            "Jaccard Score: 0.3296023564064801\n",
            "Jaccard Score: 0.3288659793814433\n",
            "Jaccard Score: 0.32709867452135494\n",
            "Jaccard Score: 0.3344624447717231\n",
            "Jaccard Score: 0.33537064310260184\n",
            "Jaccard Score: 0.33311242022582227\n",
            "Jaccard Score: 0.3378252331860579\n",
            "Jaccard Score: 0.3393225331369661\n",
            "Jaccard Score: 0.33019145802650957\n",
            "Jaccard Score: 0.33689248895434465\n",
            "Jaccard Score: 0.3332842415316642\n",
            "Jaccard Score: 0.3391016200294551\n",
            "Jaccard Score: 0.32734413352970054\n",
            "Jaccard Score: 0.33306332842415315\n",
            "Jaccard Score: 0.33598429062346585\n",
            "Jaccard Score: 0.3380706921944035\n",
            "Jaccard Score: 0.34162984781541483\n",
            "Jaccard Score: 0.34415807560137457\n",
            "Jaccard Score: 0.35434462444771725\n",
            "Jaccard Score: 0.3470544918998527\n",
            "Jaccard Score: 0.35267550319096713\n",
            "Jaccard Score: 0.34621993127147765\n",
            "Jaccard Score: 0.34342169857633775\n",
            "Jaccard Score: 0.3379725085910653\n",
            "Jaccard Score: 0.34764359351988217\n",
            "Jaccard Score: 0.3513500245459008\n",
            "Jaccard Score: 0.34889543446244475\n",
            "Jaccard Score: 0.3341188021600393\n",
            "Jaccard Score: 0.3527000490918017\n",
            "Jaccard Score: 0.35405007363770247\n",
            "Jaccard Score: 0.3435935198821797\n",
            "Jaccard Score: 0.3426607756504664\n",
            "Jaccard Score: 0.340279823269514\n",
            "Jaccard Score: 0.3310996563573883\n",
            "Jaccard Score: 0.33691703485517915\n",
            "Jaccard Score: 0.3294305351006382\n",
            "Jaccard Score: 0.32349042709867454\n",
            "Jaccard Score: 0.32282768777614135\n",
            "Jaccard Score: 0.3158811978399607\n",
            "Jaccard Score: 0.3076583210603829\n",
            "Jaccard Score: 0.3170103092783505\n",
            "Jaccard Score: 0.3154148257241041\n",
            "Jaccard Score: 0.32621502209131076\n",
            "Jaccard Score: 0.3282523318605793\n",
            "Jaccard Score: 0.3242758959253805\n",
            "Jaccard Score: 0.3208149239077074\n",
            "Jaccard Score: 0.3287432498772705\n",
            "Jaccard Score: 0.33703976435935196\n",
            "Jaccard Score: 0.3392979872361316\n",
            "Jaccard Score: 0.34295532646048105\n",
            "Jaccard Score: 0.33063328424153166\n",
            "Jaccard Score: 0.3322287677957781\n",
            "Jaccard Score: 0.3198576337751596\n",
            "Jaccard Score: 0.33618065783014234\n",
            "Jaccard Score: 0.3246195385370643\n",
            "Jaccard Score: 0.32758959253804615\n",
            "Jaccard Score: 0.3168875797741777\n",
            "Jaccard Score: 0.32243495336278843\n",
            "Jaccard Score: 0.33235149729995095\n",
            "Jaccard Score: 0.3368433971526755\n",
            "Jaccard Score: 0.32766323024054983\n",
            "Jaccard Score: 0.3258222876779578\n",
            "Jaccard Score: 0.3425380461462935\n",
            "Jaccard Score: 0.33505154639175255\n",
            "Jaccard Score: 0.34054982817869417\n",
            "Jaccard Score: 0.3383161512027491\n",
            "Jaccard Score: 0.3229504172803142\n",
            "Jaccard Score: 0.3343397152675503\n",
            "Jaccard Score: 0.33610702012763866\n",
            "Jaccard Score: 0.34042709867452137\n",
            "Jaccard Score: 0.3453853706431026\n",
            "Jaccard Score: 0.34852724594992635\n",
            "Jaccard Score: 0.33610702012763866\n",
            "Jaccard Score: 0.35027000490918014\n",
            "Jaccard Score: 0.33402061855670107\n",
            "Jaccard Score: 0.34042709867452137\n",
            "Jaccard Score: 0.3401570937653412\n",
            "Jaccard Score: 0.35311732940598917\n",
            "Jaccard Score: 0.35071183112420223\n",
            "Jaccard Score: 0.3318851251840942\n",
            "Epoch   276: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Jaccard Score: 0.3375306823760432\n",
            "Jaccard Score: 0.3293568973981345\n",
            "Jaccard Score: 0.3341924398625429\n",
            "Jaccard Score: 0.3292096219931271\n",
            "Jaccard Score: 0.34035346097201763\n",
            "Jaccard Score: 0.33762886597938147\n",
            "Jaccard Score: 0.33483063328424156\n",
            "Jaccard Score: 0.34180166912125676\n",
            "Jaccard Score: 0.32538046146293564\n",
            "Jaccard Score: 0.33497790868924887\n",
            "Jaccard Score: 0.33942071674030433\n",
            "Jaccard Score: 0.34064801178203236\n",
            "Jaccard Score: 0.3453853706431026\n",
            "Jaccard Score: 0.3351006381934217\n",
            "Jaccard Score: 0.343298969072165\n",
            "Jaccard Score: 0.34067255768286697\n",
            "Jaccard Score: 0.3427344133529701\n",
            "Jaccard Score: 0.3397889052528228\n",
            "Jaccard Score: 0.35211094747177224\n",
            "Jaccard Score: 0.34327442317133033\n",
            "Jaccard Score: 0.3470054000981836\n",
            "Jaccard Score: 0.3408443789887089\n",
            "Jaccard Score: 0.3516936671575847\n",
            "Jaccard Score: 0.3381197839960726\n",
            "Jaccard Score: 0.35505645557191945\n",
            "Jaccard Score: 0.3398134511536573\n",
            "Jaccard Score: 0.35341188021600395\n",
            "Jaccard Score: 0.3558173784977909\n",
            "Jaccard Score: 0.3490181639666176\n",
            "Jaccard Score: 0.36161021109474717\n",
            "Jaccard Score: 0.3472508591065292\n",
            "Jaccard Score: 0.35039273441335295\n",
            "Jaccard Score: 0.3391752577319588\n",
            "Jaccard Score: 0.3663721158566519\n",
            "Jaccard Score: 0.34592538046146293\n",
            "Jaccard Score: 0.354639175257732\n",
            "Jaccard Score: 0.34857633775159547\n",
            "Jaccard Score: 0.34982817869415805\n",
            "Jaccard Score: 0.347545409916544\n",
            "Jaccard Score: 0.3466372115856652\n",
            "Jaccard Score: 0.3492390770741286\n",
            "Jaccard Score: 0.34258713794796264\n",
            "Jaccard Score: 0.34933726067746684\n",
            "Jaccard Score: 0.36728031418753065\n",
            "Jaccard Score: 0.3509572901325479\n",
            "Jaccard Score: 0.34590083456062837\n",
            "Jaccard Score: 0.3513500245459008\n",
            "Jaccard Score: 0.3433726067746686\n",
            "Jaccard Score: 0.3612911143838979\n",
            "Jaccard Score: 0.3579774177712322\n",
            "Jaccard Score: 0.34921453117329404\n",
            "Jaccard Score: 0.3503190967108493\n",
            "Jaccard Score: 0.34629356897398134\n",
            "Jaccard Score: 0.345139911634757\n",
            "Jaccard Score: 0.3495336278841433\n",
            "Jaccard Score: 0.35434462444771725\n",
            "Jaccard Score: 0.356872852233677\n",
            "Jaccard Score: 0.34815905743740794\n",
            "Jaccard Score: 0.34715267550319096\n",
            "Jaccard Score: 0.3445753559155621\n",
            "Jaccard Score: 0.3434462444771723\n",
            "Jaccard Score: 0.3515218458517428\n",
            "Jaccard Score: 0.3472017673048601\n",
            "Jaccard Score: 0.3529945999018164\n",
            "Jaccard Score: 0.3487972508591065\n",
            "Jaccard Score: 0.34378988708885616\n",
            "Jaccard Score: 0.3492636229749632\n",
            "Jaccard Score: 0.3561855670103093\n",
            "Jaccard Score: 0.35343642611683845\n",
            "Jaccard Score: 0.3445508100147276\n",
            "Jaccard Score: 0.3465144820814923\n",
            "Jaccard Score: 0.3549091801669121\n",
            "Jaccard Score: 0.33888070692194405\n",
            "Jaccard Score: 0.34523809523809523\n",
            "Jaccard Score: 0.3545655375552283\n",
            "Jaccard Score: 0.3530191458026509\n",
            "Jaccard Score: 0.34661266568483057\n",
            "Jaccard Score: 0.3525527736867943\n",
            "Jaccard Score: 0.3517182130584192\n",
            "Jaccard Score: 0.35085910652920965\n",
            "Jaccard Score: 0.3423662248404516\n",
            "Jaccard Score: 0.3535591556210113\n",
            "Jaccard Score: 0.3499754540991654\n",
            "Jaccard Score: 0.35446735395189005\n",
            "Jaccard Score: 0.3452626411389298\n",
            "Jaccard Score: 0.3446489936180658\n",
            "Jaccard Score: 0.35554737358861066\n",
            "Jaccard Score: 0.35679921453117325\n",
            "Jaccard Score: 0.3515463917525773\n",
            "Jaccard Score: 0.35186548846342663\n",
            "Jaccard Score: 0.35679921453117325\n",
            "Jaccard Score: 0.34496809032891507\n",
            "Jaccard Score: 0.34702994599901815\n",
            "Jaccard Score: 0.34187530682376044\n",
            "Jaccard Score: 0.3610947471772214\n",
            "Jaccard Score: 0.3572655866470299\n",
            "Jaccard Score: 0.3622238586156112\n",
            "Jaccard Score: 0.3476681394207167\n",
            "Jaccard Score: 0.346759941089838\n",
            "Jaccard Score: 0.36379479626902306\n",
            "Jaccard Score: 0.350613647520864\n",
            "Jaccard Score: 0.35670103092783506\n",
            "Jaccard Score: 0.3597938144329897\n",
            "Jaccard Score: 0.3567746686303387\n",
            "Jaccard Score: 0.3506627393225331\n",
            "Epoch   381: reducing learning rate of group 0 to 5.0000e-05.\n",
            "Jaccard Score: 0.3610947471772214\n",
            "Jaccard Score: 0.34891998036327937\n",
            "Jaccard Score: 0.3518409425625921\n",
            "Jaccard Score: 0.35216003927344136\n",
            "Jaccard Score: 0.34803632793323513\n",
            "Jaccard Score: 0.3527736867943054\n",
            "Jaccard Score: 0.3576583210603829\n",
            "Jaccard Score: 0.3535837015218458\n",
            "Jaccard Score: 0.35179185076092295\n",
            "Jaccard Score: 0.35738831615120276\n",
            "Jaccard Score: 0.35088365243004416\n",
            "Jaccard Score: 0.3519636720667648\n",
            "Jaccard Score: 0.34970544918998525\n",
            "Jaccard Score: 0.3521109474717722\n",
            "Jaccard Score: 0.34621993127147765\n",
            "Jaccard Score: 0.357093765341188\n",
            "Jaccard Score: 0.3541973490427099\n",
            "Jaccard Score: 0.35952380952380947\n",
            "Jaccard Score: 0.3592292587137948\n",
            "Jaccard Score: 0.36038291605301914\n",
            "Jaccard Score: 0.35814923907707413\n",
            "Jaccard Score: 0.3502700049091802\n",
            "Jaccard Score: 0.3532646048109966\n",
            "Jaccard Score: 0.3621993127147766\n",
            "Jaccard Score: 0.3562346588119784\n",
            "Jaccard Score: 0.3553264604810996\n",
            "Jaccard Score: 0.36038291605301914\n",
            "Jaccard Score: 0.3540746195385371\n",
            "Jaccard Score: 0.35677466863033874\n",
            "Jaccard Score: 0.354639175257732\n",
            "Jaccard Score: 0.3507609229258713\n",
            "Jaccard Score: 0.3585419734904271\n",
            "Jaccard Score: 0.3606529209621993\n",
            "Jaccard Score: 0.3592783505154639\n",
            "Jaccard Score: 0.36168384879725085\n",
            "Jaccard Score: 0.3509081983308787\n",
            "Jaccard Score: 0.3536082474226804\n",
            "Jaccard Score: 0.3527982326951399\n",
            "Jaccard Score: 0.36062837506136475\n",
            "Jaccard Score: 0.3593519882179676\n",
            "Jaccard Score: 0.3619783996072656\n",
            "Jaccard Score: 0.3676730486008837\n",
            "Jaccard Score: 0.35326460481099653\n",
            "Jaccard Score: 0.35282277859597444\n",
            "Jaccard Score: 0.3569710358370152\n",
            "Jaccard Score: 0.3520618556701031\n",
            "Jaccard Score: 0.35346097201767307\n",
            "Jaccard Score: 0.3515709376534119\n",
            "Jaccard Score: 0.35554737358861066\n",
            "Jaccard Score: 0.3545164457535592\n",
            "Jaccard Score: 0.34995090819833086\n",
            "Jaccard Score: 0.3492145311732941\n",
            "Jaccard Score: 0.3587628865979381\n",
            "Jaccard Score: 0.36080019636720667\n",
            "Jaccard Score: 0.3594992636229749\n",
            "Jaccard Score: 0.3586647029945999\n",
            "Jaccard Score: 0.35316642120765834\n",
            "Jaccard Score: 0.37187039764359353\n",
            "Jaccard Score: 0.36224840451644574\n",
            "Jaccard Score: 0.35795287187039765\n",
            "Jaccard Score: 0.36070201276386843\n",
            "Jaccard Score: 0.355252822778596\n",
            "Jaccard Score: 0.35738831615120276\n",
            "Jaccard Score: 0.352847324496809\n",
            "Jaccard Score: 0.3612665684830633\n",
            "Jaccard Score: 0.3652184585174276\n",
            "Jaccard Score: 0.353632793323515\n",
            "Jaccard Score: 0.35500736377025033\n",
            "Jaccard Score: 0.3512027491408935\n",
            "Jaccard Score: 0.3486008836524301\n",
            "Jaccard Score: 0.3610456553755523\n",
            "Jaccard Score: 0.35920471281296024\n",
            "Jaccard Score: 0.36264113892979877\n",
            "Jaccard Score: 0.3534118802160039\n",
            "Jaccard Score: 0.3549828178694158\n",
            "Jaccard Score: 0.35466372115856654\n",
            "Jaccard Score: 0.3595729013254786\n",
            "Jaccard Score: 0.3625429553264605\n",
            "Jaccard Score: 0.35746195385370644\n",
            "Jaccard Score: 0.3584192439862543\n",
            "Jaccard Score: 0.3574619538537064\n",
            "Jaccard Score: 0.3578055964653903\n",
            "Jaccard Score: 0.3444526264113893\n",
            "Jaccard Score: 0.3622238586156112\n",
            "Jaccard Score: 0.3654393716249386\n",
            "Jaccard Score: 0.3557191948944526\n",
            "Jaccard Score: 0.36200294550810014\n",
            "Jaccard Score: 0.3458026509572901\n",
            "Jaccard Score: 0.36811487481590577\n",
            "Jaccard Score: 0.3680412371134021\n",
            "Jaccard Score: 0.3569219440353461\n",
            "Jaccard Score: 0.3572655866470299\n",
            "Jaccard Score: 0.35525282277859593\n",
            "Jaccard Score: 0.35940108001963667\n",
            "Jaccard Score: 0.36364752086401575\n",
            "Jaccard Score: 0.360505645557192\n",
            "Jaccard Score: 0.35957290132547864\n",
            "Jaccard Score: 0.359106529209622\n",
            "Jaccard Score: 0.35881197839960727\n",
            "Jaccard Score: 0.36173294059891997\n",
            "Jaccard Score: 0.3646539027982327\n",
            "Jaccard Score: 0.3599165439371625\n",
            "Jaccard Score: 0.3678939617083947\n",
            "Jaccard Score: 0.3463917525773196\n",
            "Jaccard Score: 0.3634266077565047\n",
            "Jaccard Score: 0.3610456553755523\n",
            "Jaccard Score: 0.37056946489936177\n",
            "Jaccard Score: 0.35900834560628375\n",
            "Jaccard Score: 0.35829651448208144\n",
            "Jaccard Score: 0.3659548355424644\n",
            "Jaccard Score: 0.35486008836524296\n",
            "Jaccard Score: 0.3568483063328424\n",
            "Jaccard Score: 0.3561119293078056\n",
            "Jaccard Score: 0.36362297496318113\n",
            "Jaccard Score: 0.3616593028964163\n",
            "Jaccard Score: 0.3525773195876289\n",
            "Jaccard Score: 0.3573146784486991\n",
            "Jaccard Score: 0.3693667157584683\n",
            "Jaccard Score: 0.3523318605792833\n",
            "Epoch   500: reducing learning rate of group 0 to 5.0000e-06.\n",
            "Jaccard Score: 0.3586647029945999\n",
            "Jaccard Score: 0.35311732940598917\n",
            "Jaccard Score: 0.3504909180166912\n",
            "Jaccard Score: 0.35071183112420223\n",
            "Jaccard Score: 0.3614138438880707\n",
            "Jaccard Score: 0.35365733922434955\n",
            "Jaccard Score: 0.35181639666175746\n",
            "Jaccard Score: 0.3584192439862543\n",
            "Jaccard Score: 0.36627393225331367\n",
            "Jaccard Score: 0.3531909671084929\n",
            "Jaccard Score: 0.3616593028964163\n",
            "Jaccard Score: 0.3526509572901325\n",
            "Jaccard Score: 0.36116838487972514\n",
            "Jaccard Score: 0.35738831615120276\n",
            "Jaccard Score: 0.35751104565537556\n",
            "Jaccard Score: 0.36055473735886107\n",
            "Jaccard Score: 0.36323024054982816\n",
            "Jaccard Score: 0.36575846833578796\n",
            "Jaccard Score: 0.3595483554246441\n",
            "Jaccard Score: 0.35483554246440846\n",
            "Jaccard Score: 0.3594256259204713\n",
            "Jaccard Score: 0.36399116347569954\n",
            "Jaccard Score: 0.3562346588119784\n",
            "Jaccard Score: 0.35446735395189005\n",
            "Jaccard Score: 0.3525773195876289\n",
            "Jaccard Score: 0.35591556210112907\n",
            "Jaccard Score: 0.36293568973981344\n",
            "Jaccard Score: 0.3553510063819342\n",
            "Jaccard Score: 0.3720176730486009\n",
            "Jaccard Score: 0.3516936671575847\n",
            "Jaccard Score: 0.3594992636229749\n",
            "Jaccard Score: 0.3589101620029455\n",
            "Jaccard Score: 0.36352479135984295\n",
            "Jaccard Score: 0.35913107511045655\n",
            "Jaccard Score: 0.35584192439862544\n",
            "Jaccard Score: 0.3516936671575847\n",
            "Jaccard Score: 0.35586647029946\n",
            "Jaccard Score: 0.3470054000981836\n",
            "Jaccard Score: 0.36428571428571427\n",
            "Jaccard Score: 0.35716740304369166\n",
            "Jaccard Score: 0.3578546882670594\n",
            "Jaccard Score: 0.3541973490427099\n",
            "Jaccard Score: 0.36367206676485025\n",
            "Jaccard Score: 0.35873834069710353\n",
            "Jaccard Score: 0.35981836033382425\n",
            "Jaccard Score: 0.34896907216494844\n",
            "Jaccard Score: 0.3594256259204713\n",
            "Jaccard Score: 0.3635984290623466\n",
            "Jaccard Score: 0.3563573883161512\n",
            "Jaccard Score: 0.36168384879725085\n",
            "Jaccard Score: 0.3540991654393716\n",
            "Jaccard Score: 0.3543691703485518\n",
            "Jaccard Score: 0.3566028473244968\n",
            "Jaccard Score: 0.35613647520864017\n",
            "Jaccard Score: 0.36404025527736866\n",
            "Jaccard Score: 0.35694648993618067\n",
            "Jaccard Score: 0.3584192439862543\n",
            "Jaccard Score: 0.3536573392243495\n",
            "Jaccard Score: 0.35994108983799705\n",
            "Jaccard Score: 0.3652184585174276\n",
            "Jaccard Score: 0.35579283259695627\n",
            "Jaccard Score: 0.36119293078055964\n",
            "Jaccard Score: 0.356480117820324\n",
            "Jaccard Score: 0.357486499754541\n",
            "Jaccard Score: 0.3647766323024055\n",
            "Jaccard Score: 0.35962199312714777\n",
            "Jaccard Score: 0.3618065783014237\n",
            "Jaccard Score: 0.3661266568483063\n",
            "Jaccard Score: 0.3608983799705449\n",
            "Jaccard Score: 0.36298478154148256\n",
            "Jaccard Score: 0.36585665193912614\n",
            "Jaccard Score: 0.362739322533137\n",
            "Jaccard Score: 0.34759450171821304\n",
            "Jaccard Score: 0.3657584683357879\n",
            "Jaccard Score: 0.3530191458026509\n",
            "Jaccard Score: 0.3537064310260187\n",
            "Jaccard Score: 0.35706921944035347\n",
            "Jaccard Score: 0.35682376043200786\n",
            "Jaccard Score: 0.3445262641138929\n",
            "Jaccard Score: 0.3570446735395189\n",
            "Jaccard Score: 0.3623956799214531\n",
            "Jaccard Score: 0.35459008345606285\n",
            "Jaccard Score: 0.3552773686794305\n",
            "Jaccard Score: 0.3508591065292096\n",
            "Jaccard Score: 0.37017673048600885\n",
            "Jaccard Score: 0.3532646048109966\n",
            "Jaccard Score: 0.3517427589592538\n",
            "Jaccard Score: 0.35989199803632793\n",
            "Jaccard Score: 0.35677466863033874\n",
            "Jaccard Score: 0.3588610702012764\n",
            "Epoch   590: reducing learning rate of group 0 to 5.0000e-07.\n",
            "Jaccard Score: 0.35603829160530187\n",
            "Jaccard Score: 0.34845360824742266\n",
            "Jaccard Score: 0.34747177221404024\n",
            "Jaccard Score: 0.3558173784977909\n",
            "Jaccard Score: 0.35758468335787924\n",
            "Jaccard Score: 0.3595483554246441\n",
            "Jaccard Score: 0.35900834560628375\n",
            "Jaccard Score: 0.37187039764359353\n",
            "Jaccard Score: 0.3534855179185076\n",
            "Jaccard Score: 0.3526509572901325\n",
            "Jaccard Score: 0.35969563082965145\n",
            "Jaccard Score: 0.3628375061364752\n",
            "Jaccard Score: 0.35628375061364753\n",
            "Jaccard Score: 0.3575601374570447\n",
            "Jaccard Score: 0.35446735395189005\n",
            "Jaccard Score: 0.3664212076583211\n",
            "Jaccard Score: 0.3581737849779087\n",
            "Jaccard Score: 0.3483063328424153\n",
            "Jaccard Score: 0.3505891016200295\n",
            "Jaccard Score: 0.35721649484536083\n",
            "Jaccard Score: 0.35795287187039765\n",
            "Jaccard Score: 0.36212567501227294\n",
            "Jaccard Score: 0.35049091801669124\n",
            "Jaccard Score: 0.3539518900343642\n",
            "Jaccard Score: 0.34909180166912124\n",
            "Jaccard Score: 0.365365733922435\n",
            "Jaccard Score: 0.36094747177221403\n",
            "Jaccard Score: 0.35471281296023566\n",
            "Jaccard Score: 0.3744231713303878\n",
            "Jaccard Score: 0.36229749631811486\n",
            "Jaccard Score: 0.35908198330878743\n",
            "Jaccard Score: 0.3583456062837506\n",
            "Jaccard Score: 0.35527736867943055\n",
            "Jaccard Score: 0.3641384388807069\n",
            "Jaccard Score: 0.3534855179185076\n",
            "Jaccard Score: 0.3644575355915562\n",
            "Jaccard Score: 0.364580265095729\n",
            "Jaccard Score: 0.37049582719685814\n",
            "Jaccard Score: 0.35775650466372116\n",
            "Jaccard Score: 0.36536573392243493\n",
            "Jaccard Score: 0.3519391261659303\n",
            "Jaccard Score: 0.3605792832596956\n",
            "Jaccard Score: 0.3585910652920962\n",
            "Jaccard Score: 0.35692194403534605\n",
            "Jaccard Score: 0.3630338733431517\n",
            "Jaccard Score: 0.35670103092783506\n",
            "Jaccard Score: 0.35694648993618067\n",
            "Jaccard Score: 0.35552282768777616\n",
            "Jaccard Score: 0.3646293568973981\n",
            "Jaccard Score: 0.3546882670594011\n",
            "Jaccard Score: 0.35297005400098186\n",
            "Jaccard Score: 0.35144820814923905\n",
            "Jaccard Score: 0.36087383406971035\n",
            "Jaccard Score: 0.3612174766813942\n",
            "Jaccard Score: 0.35311732940598917\n",
            "Jaccard Score: 0.3631075110456554\n",
            "Jaccard Score: 0.3554491899852725\n",
            "Jaccard Score: 0.35689739813451155\n",
            "Jaccard Score: 0.35900834560628375\n",
            "Jaccard Score: 0.350613647520864\n",
            "Jaccard Score: 0.35503190967108494\n",
            "Jaccard Score: 0.3581983308787432\n",
            "Jaccard Score: 0.3604810996563574\n",
            "Jaccard Score: 0.34882179675994107\n",
            "Jaccard Score: 0.34852724594992635\n",
            "Jaccard Score: 0.3663966617574865\n",
            "Jaccard Score: 0.37157584683357875\n",
            "Jaccard Score: 0.37091310751104567\n",
            "Jaccard Score: 0.3510309278350515\n",
            "Jaccard Score: 0.36188021600392734\n",
            "Jaccard Score: 0.365807560137457\n",
            "Jaccard Score: 0.3588610702012764\n",
            "Jaccard Score: 0.36006381934216986\n",
            "Jaccard Score: 0.3617820324005891\n",
            "Jaccard Score: 0.3623220422189494\n",
            "Jaccard Score: 0.3576092292587138\n",
            "Jaccard Score: 0.35895925380461463\n",
            "Jaccard Score: 0.37110947471772215\n",
            "Jaccard Score: 0.35390279823269516\n",
            "Jaccard Score: 0.35751104565537556\n",
            "Jaccard Score: 0.3592783505154639\n",
            "Jaccard Score: 0.35736377025036814\n",
            "Jaccard Score: 0.36350024545900833\n",
            "Jaccard Score: 0.3511291114383898\n",
            "Jaccard Score: 0.36718213058419247\n",
            "Jaccard Score: 0.3612911143838979\n",
            "Jaccard Score: 0.35336278841433477\n",
            "Jaccard Score: 0.3650711831124202\n",
            "Jaccard Score: 0.361953853706431\n",
            "Jaccard Score: 0.3643348060873834\n",
            "Epoch   680: reducing learning rate of group 0 to 5.0000e-08.\n",
            "Jaccard Score: 0.3684585174275896\n",
            "Jaccard Score: 0.35758468335787924\n",
            "Jaccard Score: 0.368213058419244\n",
            "Jaccard Score: 0.35147275405007367\n",
            "Jaccard Score: 0.3625184094256259\n",
            "Jaccard Score: 0.35490918016691214\n",
            "Jaccard Score: 0.3499263622974963\n",
            "Jaccard Score: 0.3742022582228767\n",
            "Jaccard Score: 0.35810014727540496\n",
            "Jaccard Score: 0.3618311242022582\n",
            "Jaccard Score: 0.3608738340697103\n",
            "Jaccard Score: 0.3616593028964163\n",
            "Jaccard Score: 0.35827196858124694\n",
            "Jaccard Score: 0.35522827687776143\n",
            "Jaccard Score: 0.3583701521845851\n",
            "Jaccard Score: 0.3545655375552283\n",
            "Jaccard Score: 0.3661512027491409\n",
            "Jaccard Score: 0.36897398134511533\n",
            "Jaccard Score: 0.3596465390279823\n",
            "Jaccard Score: 0.35895925380461463\n",
            "Jaccard Score: 0.3540255277368679\n",
            "Jaccard Score: 0.3598429062346588\n",
            "Jaccard Score: 0.36050564555719194\n",
            "Jaccard Score: 0.3525527736867943\n",
            "Jaccard Score: 0.35250368188512515\n",
            "Jaccard Score: 0.3620274914089347\n",
            "Jaccard Score: 0.3475699558173785\n",
            "Jaccard Score: 0.3520864015709376\n",
            "Jaccard Score: 0.35738831615120276\n",
            "Jaccard Score: 0.3606774668630338\n",
            "Jaccard Score: 0.3640893470790378\n",
            "Jaccard Score: 0.35684830633284237\n",
            "Jaccard Score: 0.356872852233677\n",
            "Jaccard Score: 0.3473490427098674\n",
            "Jaccard Score: 0.36870397643593517\n",
            "Jaccard Score: 0.36627393225331367\n",
            "Jaccard Score: 0.3590328915071183\n",
            "Jaccard Score: 0.35969563082965145\n",
            "Jaccard Score: 0.35461462935689736\n",
            "Jaccard Score: 0.35802650957290133\n",
            "Jaccard Score: 0.3572655866470299\n",
            "Jaccard Score: 0.3572901325478645\n",
            "Jaccard Score: 0.36217476681394206\n",
            "Jaccard Score: 0.3673048600883652\n",
            "Jaccard Score: 0.35733922434953364\n",
            "Jaccard Score: 0.3476926853215513\n",
            "Jaccard Score: 0.3624693176239568\n",
            "Jaccard Score: 0.35068728522336773\n",
            "Jaccard Score: 0.35613647520864017\n",
            "Jaccard Score: 0.3676975945017182\n",
            "Jaccard Score: 0.34759450171821304\n",
            "Jaccard Score: 0.3432744231713304\n",
            "Jaccard Score: 0.3676975945017182\n",
            "Jaccard Score: 0.35333824251350027\n",
            "Jaccard Score: 0.3684585174275896\n",
            "Jaccard Score: 0.3722140402552774\n",
            "Jaccard Score: 0.372287677957781\n",
            "Jaccard Score: 0.3655375552282769\n",
            "Jaccard Score: 0.3645066273932253\n",
            "Jaccard Score: 0.3635247913598429\n",
            "Jaccard Score: 0.35878743249877265\n",
            "Epoch   741: reducing learning rate of group 0 to 5.0000e-09.\n",
            "Jaccard Score: 0.3612174766813942\n",
            "Jaccard Score: 0.3618802160039274\n",
            "Jaccard Score: 0.3578301423662248\n",
            "Jaccard Score: 0.362960235640648\n",
            "Jaccard Score: 0.36062837506136475\n",
            "Jaccard Score: 0.3590328915071183\n",
            "Jaccard Score: 0.3697839960726559\n",
            "Jaccard Score: 0.3471281296023564\n",
            "Jaccard Score: 0.3545164457535591\n",
            "Jaccard Score: 0.36116838487972514\n",
            "Jaccard Score: 0.3732449680903289\n",
            "Jaccard Score: 0.355694648993618\n",
            "Jaccard Score: 0.360112911143839\n",
            "Jaccard Score: 0.3533627884143348\n",
            "Jaccard Score: 0.34899361806578305\n",
            "Jaccard Score: 0.35846833578792336\n",
            "Jaccard Score: 0.3475454099165439\n",
            "Jaccard Score: 0.3563819342169857\n",
            "Jaccard Score: 0.35996563573883167\n",
            "Jaccard Score: 0.3606038291605302\n",
            "Jaccard Score: 0.3624447717231223\n",
            "Jaccard Score: 0.34398625429553265\n",
            "Jaccard Score: 0.36067746686303387\n",
            "Jaccard Score: 0.3616593028964163\n",
            "Jaccard Score: 0.3638193421698576\n",
            "Jaccard Score: 0.3532155130093274\n",
            "Jaccard Score: 0.3606529209621993\n",
            "Jaccard Score: 0.3503190967108493\n",
            "Jaccard Score: 0.3561610211094748\n",
            "Jaccard Score: 0.35802650957290133\n",
            "Jaccard Score: 0.36180657830142365\n",
            "Jaccard Score: 0.35149729995090817\n",
            "Jaccard Score: 0.359499263622975\n",
            "Jaccard Score: 0.3557191948944526\n",
            "Jaccard Score: 0.36080019636720667\n",
            "Jaccard Score: 0.3572901325478645\n",
            "Jaccard Score: 0.3538291605301915\n",
            "Jaccard Score: 0.3618802160039273\n",
            "Jaccard Score: 0.35876288659793815\n",
            "Jaccard Score: 0.3509327442317133\n",
            "Jaccard Score: 0.3669612174766814\n",
            "Jaccard Score: 0.3637211585665194\n",
            "Jaccard Score: 0.3647766323024055\n",
            "Jaccard Score: 0.3594747177221404\n",
            "Jaccard Score: 0.35503190967108494\n",
            "Jaccard Score: 0.3719194894452626\n",
            "Jaccard Score: 0.34783996072655865\n",
            "Jaccard Score: 0.350613647520864\n",
            "Jaccard Score: 0.3534118802160039\n",
            "Jaccard Score: 0.35989199803632793\n",
            "Jaccard Score: 0.3538291605301915\n",
            "Jaccard Score: 0.3640648011782032\n",
            "Jaccard Score: 0.36610211094747175\n",
            "Jaccard Score: 0.34661266568483057\n",
            "Jaccard Score: 0.34796269023073145\n",
            "Jaccard Score: 0.3605792832596956\n",
            "Jaccard Score: 0.34658811978399606\n",
            "Jaccard Score: 0.3634756995581738\n",
            "Jaccard Score: 0.3575110456553755\n",
            "Jaccard Score: 0.3624693176239568\n",
            "Jaccard Score: 0.34879725085910657\n",
            "Jaccard Score: 0.366813942071674\n",
            "Jaccard Score: 0.34378988708885616\n",
            "Jaccard Score: 0.3586892488954344\n",
            "Jaccard Score: 0.3660039273441335\n",
            "Jaccard Score: 0.36040746195385365\n",
            "Jaccard Score: 0.363966617574865\n",
            "Jaccard Score: 0.3556210112911144\n",
            "Jaccard Score: 0.3575601374570446\n",
            "Jaccard Score: 0.35974472263132057\n",
            "Jaccard Score: 0.36440844378988707\n",
            "Jaccard Score: 0.35515463917525775\n",
            "Jaccard Score: 0.36001472754050073\n",
            "Jaccard Score: 0.36168384879725085\n",
            "Jaccard Score: 0.3625184094256259\n",
            "Jaccard Score: 0.36229749631811486\n",
            "Jaccard Score: 0.35932744231713304\n",
            "Jaccard Score: 0.3616593028964163\n",
            "Jaccard Score: 0.35547373588610703\n",
            "Jaccard Score: 0.35503190967108494\n",
            "Jaccard Score: 0.36119293078055964\n",
            "Jaccard Score: 0.35287187039764356\n",
            "Jaccard Score: 0.3595238095238095\n",
            "Jaccard Score: 0.36190476190476195\n",
            "Jaccard Score: 0.3515463917525773\n",
            "Jaccard Score: 0.3574128620520373\n",
            "Jaccard Score: 0.36308296514482086\n",
            "Jaccard Score: 0.36082474226804123\n",
            "Jaccard Score: 0.35149729995090817\n",
            "Jaccard Score: 0.3620274914089347\n",
            "Jaccard Score: 0.36148748159057437\n",
            "Jaccard Score: 0.346367206676485\n",
            "Jaccard Score: 0.3577810505645557\n",
            "Jaccard Score: 0.3635247913598429\n",
            "Jaccard Score: 0.34977908689248893\n",
            "Jaccard Score: 0.34653902798232694\n",
            "Jaccard Score: 0.352847324496809\n",
            "Jaccard Score: 0.3583701521845851\n",
            "Jaccard Score: 0.35959744722631326\n",
            "Jaccard Score: 0.35574374079528714\n",
            "Jaccard Score: 0.35844378988708886\n",
            "Jaccard Score: 0.3575110456553755\n",
            "Jaccard Score: 0.3495827196858125\n",
            "Jaccard Score: 0.3572164948453608\n",
            "Jaccard Score: 0.36018654884634266\n",
            "Jaccard Score: 0.3603583701521846\n",
            "Jaccard Score: 0.35527736867943055\n",
            "Jaccard Score: 0.36769759450171824\n",
            "Jaccard Score: 0.3545409916543937\n",
            "Jaccard Score: 0.3561610211094747\n",
            "Jaccard Score: 0.35613647520864017\n",
            "Jaccard Score: 0.3640648011782033\n",
            "Jaccard Score: 0.3565537555228277\n",
            "Jaccard Score: 0.353853706431026\n",
            "Jaccard Score: 0.35488463426607764\n",
            "Jaccard Score: 0.3552282768777614\n",
            "Jaccard Score: 0.3633284241531664\n",
            "Jaccard Score: 0.3537800687285223\n",
            "Jaccard Score: 0.3621993127147766\n",
            "Jaccard Score: 0.35486008836524296\n",
            "Jaccard Score: 0.35540009818360335\n",
            "Jaccard Score: 0.3539273441335297\n",
            "Jaccard Score: 0.3555228276877761\n",
            "Jaccard Score: 0.35613647520864017\n",
            "Jaccard Score: 0.359720176730486\n",
            "Jaccard Score: 0.35839469808541974\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WNBFOGtJ7zA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = lambda x: P(F(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFkYxjMrRPNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_label_tensors = get_label_tensors(en_testdata)\n",
        "y_true = torch.stack(test_label_tensors)\n",
        "\n",
        "test_dataset = TensorDataset(torch.from_numpy(en_test_embeddings), y_true)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=100, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfelwQOuJ80u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "THRESHOLD = 0.5\n",
        "print(\"Threshold:\", THRESHOLD)\n",
        "up = 0\n",
        "\n",
        "all_list = []\n",
        "\n",
        "for x, y in test_dataloader:\n",
        "    guess = net(x)\n",
        "    all_list.append(guess)\n",
        "    # print(guess)\n",
        "    # values, indices = torch.topk(guess, 2)\n",
        "    # print(\"values:\", values)\n",
        "    # print(\"indices:\", indices)\n",
        "    for i in range(x.shape[0]):\n",
        "        for j in range(11):\n",
        "            if (guess[i][j] >= THRESHOLD and y[i][j] == 1) or (guess[i][j] < THRESHOLD and y[i][j] == 0):\n",
        "              up += 1\n",
        "\n",
        "concat_tensor = torch.cat(all_list)\n",
        "y_score = concat_tensor.detach().numpy()\n",
        "\n",
        "print(\"ACCURACY:\", up / (len(test_label_tensors)*11))\n",
        "\n",
        "from sklearn.metrics import f1_score, jaccard_score, hamming_loss\n",
        "y_pred = y_score.copy()\n",
        "for i in range(y_pred.shape[0]):\n",
        "  for j in range(y_pred.shape[1]):\n",
        "    if (y_pred[i][j] >= THRESHOLD):\n",
        "      y_pred[i][j] = 1\n",
        "    else:\n",
        "      y_pred[i][j] = 0\n",
        "f1score = f1_score(y_true, y_pred, average=None)\n",
        "print(\"F1 Score:\", f1score)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "print(\"F1 Micro Score:\", f1_micro)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "print(\"F1 Macro Score:\", f1_macro)\n",
        "jac = jaccard_score(y_true, y_pred, average='samples')\n",
        "print(\"Jaccard Score:\", jac)\n",
        "# print(y_pred)\n",
        "\n",
        "lrap_score = label_ranking_average_precision_score(y_true, y_score)\n",
        "print(\"LRAP Score:\", lrap_score)\n",
        "\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(\"Hamming Loss:\", hl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YgBbcHNZCGd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(net.state_dict(), savepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1rVEs_zR__e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lrap_score = label_ranking_average_precision_score(y_true, y_score)\n",
        "print(\"LRAP Score:\", lrap_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5R5ZNFZBBIh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import f1_score, jaccard_score, hamming_loss\n",
        "y_pred = y_score.copy()\n",
        "for i in range(y_pred.shape[0]):\n",
        "  for j in range(y_pred.shape[1]):\n",
        "    if (y_pred[i][j] >= 0):\n",
        "      y_pred[i][j] = 1\n",
        "    else:\n",
        "      y_pred[i][j] = 0\n",
        "f1score = f1_score(y_true, y_pred, average=None)\n",
        "print(\"F1 Score:\", f1score)\n",
        "f1_micro = f1_score(y_true, y_pred, average='micro')\n",
        "print(\"F1 Micro Score:\", f1_micro)\n",
        "f1_macro = f1_score(y_true, y_pred, average='macro')\n",
        "print(\"F1 Macro Score:\", f1_macro)\n",
        "jac = jaccard_score(y_true, y_pred, average='samples')\n",
        "print(\"Jaccard Score:\", jac)\n",
        "print(y_pred)\n",
        "\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(\"Hamming Loss:\", hl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxL0t_TTeb6a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import hamming_loss\n",
        "hl = hamming_loss(y_true, y_pred)\n",
        "print(hl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSVC7lU-j3UR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(P.state_dict(), '/content/P_es_0713.model')\n",
        "torch.save(F.state_dict(), '/content/F_es_0713.model')\n",
        "torch.save(Q.state_dict(), '/content/Q_es_0713.model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}